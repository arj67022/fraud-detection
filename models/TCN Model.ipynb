{"cells":[{"cell_type":"markdown","metadata":{"id":"iV_MG7NYIxuv"},"source":["# Dataloader Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhaCJvG3Qbz0"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import os\n","import gensim\n","import torch.nn.functional as F\n","from sklearn.metrics import f1_score, precision_score, recall_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJEQldSgQdnB"},"outputs":[],"source":["CONSTANTS = {\n","    'label': 'default payment next month',\n","    'path': 'preprocessed_upsampled.csv',\n","    'sequence_features': ['PAY_', 'BILL_AMT', 'PAY_AMT', 'Avg_exp_', 'Client_', 'Closeness_'],\n","    'non_sequence_features': ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE'],\n","    'length': 6,\n","    'batch_size': 256,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gopXiZ1zQgGJ"},"outputs":[],"source":["class Dataset_seq(Dataset):\n","  def __init__(self, path):\n","    self.data = pd.read_csv(path)\n","    self.label = CONSTANTS['label']\n","    self.features = list(self.data.columns)\n","    self.features.remove(self.label)\n","  \n","  def __getitem__(self, index):\n","    ex = self.data.iloc[index]\n","    label = ex[self.label]\n","    features = ex[self.features]\n","\n","    non_sequential_features = list(features[CONSTANTS['non_sequence_features']].values)\n","\n","    all_features = []\n","\n","    for i in range(CONSTANTS['length'], 0, -1):\n","      seq_i = []\n","      for base_feature in CONSTANTS['sequence_features']:\n","        seq_i.append(features[f'{base_feature}{i}'])\n","      \n","      all_features.append(seq_i)\n","    \n","    non_sequential_features = torch.tensor(non_sequential_features, dtype=torch.float)\n","    all_features = np.array(all_features)\n","    all_features = all_features.astype(np.double)\n","    all_features = torch.from_numpy(all_features)\n","\n","    all_features = all_features.type(torch.int64)\n","    all_features = torch.nn.functional.one_hot(all_features, 10).view((all_features.shape[0], -1))\n","    all_features = all_features.type(torch.float)\n","    # all_features = torch.tensor(all_features, dtype=torch.float)\n","\n","    labels = torch.tensor(label, dtype=torch.int64)\n","    labels = torch.nn.functional.one_hot(labels, 2)\n","    labels = torch.tensor(labels, dtype=torch.int64)\n","\n","    return all_features, non_sequential_features, labels\n","  \n","  def __len__(self):\n","    return self.data.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NphVNFGeME3l"},"outputs":[],"source":["seq_dataset = Dataset_seq(CONSTANTS['path'])\n","train_data, val_data = torch.utils.data.random_split(seq_dataset, [37425, 9303])\n","train_loader = DataLoader(train_data, batch_size=CONSTANTS['batch_size'], shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=CONSTANTS['batch_size'], shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"XLONYVanK3YT"},"source":["# Train Functions"]},{"cell_type":"code","source":["def metrics(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n","    '''Calculate F1 score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. 0 <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    \n","    '''\n","    assert y_true.ndim == 1\n","    assert y_pred.ndim == 1 or y_pred.ndim == 2\n","    \n","    if y_pred.ndim == 2:\n","        y_pred = y_pred.argmax(dim=1)\n","        \n","    \n","    tp = (y_true * y_pred).sum().to(torch.float32)\n","    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n","    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n","    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n","    \n","    epsilon = 1e-7\n","    \n","    precision = tp / (tp + fp + epsilon)\n","    recall = tp / (tp + fn + epsilon)\n","    \n","    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n","    f1.requires_grad = is_training\n","    \n","    return torch.round(precision, decimals=3), torch.round(recall, decimals=3), torch.round(f1, decimals=3)"],"metadata":{"id":"VpS6vv81nbPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUZBxDw8LK4e"},"outputs":[],"source":["from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","def train(model, optimizer, lr_scheduler, criterion, train_loader, test_loader, epochs):\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  model = model.to(device)\n","  n = len(train_loader)\n","  train_loss, test_loss = [], []\n","  train_acc, test_acc = [], []\n","  train_f1, test_f1 = [], []\n","\n","  for epoch in range(epochs):\n","    correct = 0\n","    num_examples = 0\n","    running_loss = 0\n","    model.train()\n","    preds, labels = [], []\n","    for i, (seq, non_seq, target) in enumerate(train_loader):\n","      seq = seq.to(device)\n","      target = target.to(device)\n","      non_seq = non_seq.to(device)\n","      \n","      optimizer.zero_grad()\n","      output = model(seq, non_seq)\n","      loss = criterion(output, target.float())\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","      optimizer.step()\n","      _, pred = torch.max(output, 1)\n","      _, target = torch.max(target, 1)\n","      preds.extend(list(pred.detach().cpu()))\n","      labels.extend(target.tolist())\n","      correct += (pred == target).float().detach().cpu().sum()\n","      num_examples += len(output)\n","      running_loss += loss.item()\n","\n","    accuracy = correct / num_examples\n","    precision, recall, f1 = metrics(torch.tensor(preds), torch.tensor(labels))\n","    print (f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader)}, Accuracy:{accuracy} Precision: {precision} Recall: {recall} F1: {f1}')\n","    train_loss.append(running_loss / len(train_loader))\n","    train_acc.append(accuracy)\n","    train_f1.append(f1)\n","\n","    model.eval()\n","    correct = 0\n","    num_examples = 0\n","    running_loss = 0\n","    precision, recall, f1 = 0, 0, 0\n","    with torch.no_grad():\n","      for i, (seq, non_seq, target) in enumerate(test_loader):\n","        seq = seq.to(device)\n","        target = target.to(device)\n","        non_seq = non_seq.to(device)\n","        output = model(seq, non_seq)\n","        optimizer.zero_grad()\n","        loss = criterion(output, target.float())\n","        _, pred = torch.max(output, 1)\n","        _, target = torch.max(target, 1)\n","        correct += (pred == target).float().detach().cpu().sum()\n","        num_examples += len(target)\n","        running_loss += loss.item()\n","        preds.extend(list(pred.detach().cpu()))\n","        labels.extend(target.tolist())\n","\n","    lr_scheduler.step(loss.item())\n","    accuracy = correct / num_examples\n","    precision, recall, f1 = metrics(torch.tensor(preds), torch.tensor(labels))\n","    print (f'Test Loss: {running_loss / len(test_loader)}, Test Accuracy:{accuracy} Test Precision: {precision} Test Recall: {recall} Test F1: {f1}')\n","    test_loss.append(running_loss / len(test_loader))\n","    test_acc.append(accuracy)\n","    test_f1.append(f1)\n","\n","  return train_loss, test_loss, train_acc, test_acc, train_f1, test_f1"]},{"cell_type":"markdown","metadata":{"id":"ofirBWJqI1I7"},"source":["# TCN"]},{"cell_type":"code","source":["import math\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","from torch.nn.utils import weight_norm\n","from torch.autograd import Variable\n","\n","# code used from: https://github.com/flrngel/TCN-with-attention\n","\n","class Chomp1d(nn.Module):\n","  def __init__(self, chomp_size):\n","    super(Chomp1d, self).__init__()\n","    self.chomp_size = chomp_size\n","\n","  def forward(self, x):\n","    return x[:, :, :-self.chomp_size].contiguous()\n","\n","\n","class TemporalBlock(nn.Module):\n","  def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2, bn):\n","    super(TemporalBlock, self).__init__()\n","\n","    if bn:\n","      self.norm1 = nn.BatchNorm1d(n_outputs)\n","      self.norm2 = nn.BatchNorm1d(n_outputs)\n","    else:\n","      self.norm1 = weight_norm()\n","      self.norm2 = weight_norm()\n","\n","    self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n","                                       stride=stride, padding=padding, dilation=dilation)\n","    self.chomp1 = Chomp1d(padding)\n","    self.relu1 = nn.LeakyReLU()\n","    self.dropout1 = nn.Dropout2d(dropout)\n","\n","    self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n","                                       stride=stride, padding=padding, dilation=dilation)\n","    self.chomp2 = Chomp1d(padding)\n","    self.relu2 = nn.LeakyReLU()\n","    self.dropout2 = nn.Dropout2d(dropout)\n","\n","    self.net = nn.Sequential(self.conv1, self.norm1, self.chomp1, self.relu1, self.dropout1,\n","                             self.conv2, self.norm2, self.chomp2, self.relu2, self.dropout2)\n","    self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","    self.relu = nn.ReLU()\n","    self.init_weights()\n","\n","  def init_weights(self):\n","    nn.init.xavier_uniform_(self.conv1.weight, gain=np.sqrt(2))\n","    nn.init.xavier_uniform_(self.conv2.weight, gain=np.sqrt(2))\n","\n","    if self.downsample is not None:\n","        nn.init.xavier_uniform_(self.downsample.weight, gain=np.sqrt(2))\n","\n","  def forward(self, x):\n","    net = self.net(x)\n","    res = x if self.downsample is None else self.downsample(x)\n","    return self.relu(net + res)\n","\n","\n","class AttentionBlock(nn.Module):\n","  def __init__(self, dims, k_size, v_size, seq_len=None):\n","    super(AttentionBlock, self).__init__()\n","    self.key_layer = nn.Linear(dims, k_size)\n","    self.query_layer = nn.Linear(dims, k_size)\n","    self.value_layer = nn.Linear(dims, v_size)\n","    self.sqrt_k = math.sqrt(k_size)\n","\n","  def forward(self, minibatch):\n","    keys = self.key_layer(minibatch)\n","    queries = self.query_layer(minibatch)\n","    values = self.value_layer(minibatch)\n","    logits = torch.bmm(queries, keys.transpose(2,1))\n","    mask = np.triu(np.ones(logits.size()), k=1).astype('bool')\n","    mask = torch.from_numpy(mask).cuda()\n","    logits.data.masked_fill_(mask, False)\n","    probs = F.softmax(logits, dim=1) / self.sqrt_k\n","    read = torch.bmm(probs, values)\n","    return minibatch + read\n","\n","class TemporalConvNet(nn.Module):\n","  def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2, max_length=60, attention=False, bn=False):\n","    super(TemporalConvNet, self).__init__()\n","    layers = []\n","    num_levels = len(num_channels)\n","    for i in range(num_levels):\n","      dilation_size = 2 ** i\n","      in_channels = num_inputs if i == 0 else num_channels[i-1]\n","      out_channels = num_channels[i]\n","      layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n","                               padding=(kernel_size-1) * dilation_size, dropout=dropout, bn=bn)]\n","      if attention == True:\n","        layers += [AttentionBlock(max_length, max_length, max_length)]\n","\n","    self.network = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    x = self.network(x)\n","    return x\n","\n","class TCNModel(nn.Module):\n","  def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2, attention=False):\n","    super(TCNModel, self).__init__()\n","    self.tcn = TemporalConvNet(num_inputs, num_channels, kernel_size, dropout, attention=attention)\n","    # self.pool = nn.AvgPool2d(2)\n","    self.norm = nn.LayerNorm(5)\n","    self.dropout = nn.Dropout(dropout)\n","\n","    self.decoder = nn.Sequential(\n","        nn.Linear(261, 2),\n","    )\n","    \n","  def forward(self, seq, non_seq):\n","    x = self.tcn(seq)\n","    x = self.dropout(x[:, :, -1])\n","    x = torch.cat([x, self.norm(non_seq)], dim=-1)\n","    x = self.decoder(x)\n","\n","    return F.softmax(x, dim=-1)"],"metadata":{"id":"KKvXts1_-3sI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Ib2Ri53CrVE"},"source":["# Multimodal TCN with Attention Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSWAT7uaDFVs"},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSCdWY2aCuKt"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = TCNModel(num_inputs = 6, num_channels=[64, 64, 128, 128, 256, 256], kernel_size=3, dropout=0.50, attention=True)\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=1e-3, weight_decay=.0001)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n","criterion = nn.CrossEntropyLoss()\n","epochs = 15\n","train_loss, test_loss, train_acc, test_acc, train_f1, test_f1 = train(model, optimizer, lr_scheduler, criterion, train_loader, val_loader, epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVKBkZEWEt_-"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.semilogy(np.array(range(len(train_loss))), train_loss)\n","plt.semilogy(np.array(range(len(test_loss))), test_loss)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('Training loss')\n","plt.xlabel('Epoch')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-F2ZQAlEv0H"},"outputs":[],"source":["plt.semilogy(np.array(range(len(train_acc))), train_acc)\n","plt.semilogy(np.array(range(len(test_acc))), test_acc)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.show()"]},{"cell_type":"code","source":["plt.semilogy(np.array(range(len(train_acc))), train_f1)\n","plt.semilogy(np.array(range(len(test_acc))), test_f1)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('F1')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"F83WmjXkr6vG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multimodal TCN without Attention Training"],"metadata":{"id":"qOJFKRcDjUBO"}},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = TCNModel(num_inputs = 6, num_channels=[32, 32, 64, 64, 128, 128, 256, 256], kernel_size=3, dropout=0.50, attention=False)\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=1e-3, weight_decay=.0001)\n","lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.00001, threshold_mode='abs')\n","criterion = nn.CrossEntropyLoss()\n","epochs = 15\n","train_loss, test_loss, train_acc, test_acc, train_f1, test_f1 = train(model, optimizer, lr_scheduler, criterion, train_loader, val_loader, epochs)"],"metadata":{"id":"ALUGyQsmjW-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.semilogy(np.array(range(len(train_loss))), train_loss)\n","plt.semilogy(np.array(range(len(test_loss))), test_loss)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('Training loss')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"czMWTFBljcX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.semilogy(np.array(range(len(train_acc))), train_acc)\n","plt.semilogy(np.array(range(len(test_acc))), test_acc)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"Lhx88w1qjc-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.semilogy(np.array(range(len(train_acc))), train_f1)\n","plt.semilogy(np.array(range(len(test_acc))), test_f1)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('F1')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"yhw4yrzLsDsN"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["iV_MG7NYIxuv","XLONYVanK3YT","ofirBWJqI1I7","qOJFKRcDjUBO"],"provenance":[{"file_id":"1g2OkCbr0h7erx_MLror04YyDHoNl6kU0","timestamp":1670996046069}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}