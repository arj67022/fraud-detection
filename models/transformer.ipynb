{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"GhaCJvG3Qbz0","executionInfo":{"status":"ok","timestamp":1671357433453,"user_tz":-240,"elapsed":4288,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import numpy as np\n","import os\n","import gensim\n","import time\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()"],"metadata":{"id":"H-2G7bN7Lko8","executionInfo":{"status":"ok","timestamp":1671357433453,"user_tz":-240,"elapsed":7,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xJEQldSgQdnB","executionInfo":{"status":"ok","timestamp":1671357433453,"user_tz":-240,"elapsed":6,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"outputs":[],"source":["CONSTANTS = {\n","    'label': 'default payment next month',\n","    'path': 'ppppreprocessed (1).csv' , #\n","    'sequence_features': ['PAY_', 'BILL_AMT', 'PAY_AMT'],\n","    'non_sequence_features': ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE'],\n","    'length': 6,\n","    'batch_size': 64,\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gopXiZ1zQgGJ","executionInfo":{"status":"ok","timestamp":1671357433454,"user_tz":-240,"elapsed":7,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"outputs":[],"source":["class Dataset_seq(Dataset):\n","  def __init__(self, path):\n","    self.data = pd.read_csv(path)\n","    self.label = CONSTANTS['label']\n","    self.features = list(self.data.columns)\n","    self.features.remove(self.label)\n","  \n","  def __getitem__(self, index):\n","    ex = self.data.iloc[index]\n","    label = ex[self.label]\n","    features = ex[self.features]\n","\n","    non_sequential_features = list(features[CONSTANTS['non_sequence_features']].values)\n","\n","    all_features = []\n","\n","    for i in range(1, 1 + CONSTANTS['length']):\n","      seq_i = []\n","      for base_feature in CONSTANTS['sequence_features']:\n","        seq_i.append(features[f'{base_feature}{i}'])\n","      \n","      seq_i += non_sequential_features\n","      all_features.append(seq_i)\n","    \n","    all_features = np.array(all_features)\n","    all_features = all_features.astype(np.double)\n","    all_features = torch.from_numpy(all_features)\n","\n","    all_features = all_features.type(torch.float)\n","\n","    return torch.flatten(all_features), torch.tensor(label, dtype=torch.long)\n","  \n","  def __len__(self):\n","    return self.data.shape[0]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1pF78-gbDUF","outputId":"e2db677e-7338-4630-8a34-8e2c65dcd88b","executionInfo":{"status":"ok","timestamp":1671357456638,"user_tz":-240,"elapsed":23190,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"43MtDHjsfxny","executionInfo":{"status":"ok","timestamp":1671357456639,"user_tz":-240,"elapsed":6,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"outputs":[],"source":["data = Dataset_seq(CONSTANTS['path'])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"BHe7jF2PcN0f","executionInfo":{"status":"ok","timestamp":1671357456639,"user_tz":-240,"elapsed":5,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1627a1a-4d4c-4483-ea08-6b3bd369c874"},"outputs":[{"output_type":"stream","name":"stdout","text":["20758\n","16606 4151 20757\n"]}],"source":["# Training \n","from collections import Counter\n","batch_sz = CONSTANTS['batch_size']\n","\n","data_size = len(data)\n","print(len(data))\n","print(int(len(data)*0.80), int(len(data)*0.20), int(len(data)*0.80) + int(len(data)*0.20))\n","train_set, valid_set = random_split(data, [int(len(data)*0.80)+1, int(len(data)*0.20)])\n","\n","train_loader = DataLoader(train_set, batch_size=batch_sz, shuffle=True)\n","test_loader = DataLoader(valid_set, batch_size=batch_sz, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"alinFuFPcJnP"},"source":["# Model"]},{"cell_type":"code","source":["d = pd.read_csv(CONSTANTS['path']).drop(['default payment next month', 'ID'], axis=1)\n","unique_tokens = pd.unique(d[d.columns].values.ravel('K'))\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using {} device\".format(device))"],"metadata":{"id":"3e4k5iw4Iu17","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671357457084,"user_tz":-240,"elapsed":448,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}},"outputId":"49ea4506-fdd8-49a6-f655-cc8f0f24052a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}]},{"cell_type":"code","source":["def metrics(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n","    '''Calculate F1 score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. 0 <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    \n","    '''\n","    assert y_true.ndim == 1\n","    assert y_pred.ndim == 1 or y_pred.ndim == 2\n","    \n","    if y_pred.ndim == 2:\n","        y_pred = y_pred.argmax(dim=1)\n","        \n","    \n","    tp = (y_true * y_pred).sum().to(torch.float32)\n","    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n","    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n","    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n","    \n","    epsilon = 1e-7\n","    \n","    precision = tp / (tp + fp + epsilon)\n","    recall = tp / (tp + fn + epsilon)\n","    \n","    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n","    f1.requires_grad = is_training\n","    \n","    return torch.round(precision, decimals=3), torch.round(recall, decimals=3), torch.round(f1, decimals=3)"],"metadata":{"id":"nU0IpmnmLehj","executionInfo":{"status":"ok","timestamp":1671357457085,"user_tz":-240,"elapsed":3,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Tu7eh8X_cLK5","executionInfo":{"status":"ok","timestamp":1671357462938,"user_tz":-240,"elapsed":5856,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class ClassificationTransformer(nn.Module):\n","  def __init__(self, input_dim, output_dim, hidden_dim, n_layers, n_heads, dropout, max_sequence_length, device):\n","    super().__init__()\n","\n","    self.embedding = nn.Embedding(input_dim, hidden_dim)\n","    self.encoder_layers = nn.TransformerEncoderLayer(hidden_dim, n_heads, hidden_dim, dropout)\n","    self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers, n_layers)\n","    self.fc = nn.Linear(hidden_dim, output_dim)\n","    self.positional_encoding = self.create_positional_encoding(max_sequence_length, hidden_dim, device)\n","    self.to(device)\n","\n","  def create_positional_encoding(self, max_sequence_length, hidden_dim, device):\n","    positional_encoding = torch.zeros(max_sequence_length, hidden_dim, device=device)\n","    alpha = 1 / (10000 ** (torch.arange(0, hidden_dim, 2, dtype=torch.float, device=device) / hidden_dim))\n","    positional_encoding[:, 0::2] = torch.sin(torch.arange(0, max_sequence_length, device=device).unsqueeze(1) * alpha)\n","    positional_encoding[:, 1::2] = torch.cos(torch.arange(0, max_sequence_length, device=device).unsqueeze(1) * alpha)\n","    return positional_encoding\n","\n","  def add_positional_encoding(self, x):\n","    batch_size, sequence_length = x.shape[:2]\n","    x = x.to(self.positional_encoding.device)\n","    positional_encoding = self.positional_encoding[:sequence_length]\n","    positional_encoding = positional_encoding.repeat(batch_size, 1, 1)\n","    return x + positional_encoding\n","\n","  def forward(self, x):\n","    x = x.to(self.positional_encoding.device)\n","    x = self.add_positional_encoding(self.embedding(x))\n","    x = self.transformer_encoder(x)\n","    x = x.mean(dim=1)\n","    return self.fc(x)\n","\n","model = ClassificationTransformer(len(unique_tokens), 2, hidden_dim=320, n_layers=5, n_heads=8, dropout=0.2, max_sequence_length=100, device=device).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), lr=0.1, weight_decay=.0001)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n","epochs = 10"]},{"cell_type":"code","source":["#train\n","train_total_correct = []\n","train_total_loss = []\n","train_precision = []\n","train_recall = []\n","train_f1 = []\n","val_total_correct = []\n","val_total_loss = []\n","val_precision = []\n","val_recall = []\n","val_f1 = []\n","\n","for epoch in range(epochs):\n","  model.train()\n","  precision, recall, f1 = 0, 0, 0\n","  preds, labelss = [], []\n","  total_loss = 0\n","  total_correct = 0\n","  for inputs, labels in train_loader:\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    optimizer.zero_grad()\n","    outputs = model(inputs.to(torch.int64))\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","\n","    total_loss += loss.item()\n","    total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","    _, pred = torch.max(outputs, 1)\n","    preds.extend([g.item() for g in list(pred.detach().cpu())])\n","    labelss.extend(labels.tolist())\n","  \n","  lr_scheduler.step()\n","  precision, recall, f1 = metrics(torch.tensor(preds), torch.tensor(labelss))\n","\n","  train_total_correct.append(total_correct / len(train_loader))\n","  train_total_loss.append(total_loss / len(train_loader))\n","  train_precision.append(precision)\n","  train_recall.append(recall)\n","  train_f1.append(f1)\n","\n","  print(f\"\"\"TRAIN --> Epoch {epoch+1}: loss = {total_loss / len(train_loader):.4f}, accuracy = {total_correct / len(train_loader):.4f}, precision = {precision:.4f},, recall = {recall:.4f}, f1 = {f1:.4f}\"\"\")\n","\n","  with torch.no_grad():\n","    precision, recall, f1 = 0, 0, 0\n","    preds, labelss = [], []\n","    model.eval()\n","    total_loss = 0\n","    total_correct = 0\n","    for inputs, labels in test_loader:\n","      inputs, labels = inputs.to(device), labels.to(device)\n","      outputs = model(inputs.to(torch.int64))\n","      total_loss += criterion(outputs, labels).item()\n","      total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","      _, pred = torch.max(outputs, 1)\n","      preds.extend(list(pred.detach().cpu()))\n","      labelss.extend(labels.tolist())\n","    \n","    precision, recall, f1 = metrics(torch.tensor(preds), torch.tensor(labelss))\n","    val_total_correct.append(total_correct / len(test_loader))\n","    val_total_loss.append(total_loss / len(test_loader))\n","    val_precision.append(precision)\n","    val_recall.append(recall)\n","    val_f1.append(f1)\n","\n","  print(f\"\"\"VAL --> Epoch {epoch+1}: loss = {total_loss / len(test_loader):.4f}, accuracy = {total_correct / len(test_loader):.4f}, precision = {precision:.4f},, recall = {recall:.4f}, f1 = {f1:.4f}\"\"\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVoY6Eb2xgj6","executionInfo":{"status":"ok","timestamp":1671358334761,"user_tz":-240,"elapsed":639,"user":{"displayName":"Aditya Rathi","userId":"00300668789215840460"}},"outputId":"89bfad14-962f-4d21-8fb5-9aaa4d233aad"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["TRAIN --> Epoch 1, loss = 0.4470, accuracy = 0.7765, precision = 0.7577, recall = 0.8780, f1 = 0.8022\n","VAL --> Epoch 1, loss = 0.4682, accuracy = 0.7861, precision = 0.7576, recall = 0.8568, f1 = 0.8077\n","==============================\n","TRAIN --> Epoch 2, loss = 0.4492, accuracy = 0.7861, precision = 0.7594, recall = 0.8796, f1 = 0.8019\n","VAL --> Epoch 2, loss = 0.4681, accuracy = 0.7811, precision = 0.7586, recall = 0.8663, f1 = 0.8047\n","==============================\n","TRAIN --> Epoch 3, loss = 0.4499, accuracy = 0.7955, precision = 0.7584, recall = 0.8780, f1 = 0.8045\n","VAL --> Epoch 3, loss = 0.4541, accuracy = 0.7916, precision = 0.7592, recall = 0.8680, f1 = 0.8154\n","==============================\n","TRAIN --> Epoch 4, loss = 0.4497, accuracy = 0.7918, precision = 0.7620, recall = 0.8827, f1 = 0.8115\n","VAL --> Epoch 4, loss = 0.4417, accuracy = 0.7985, precision = 0.7622, recall = 0.8764, f1 = 0.8272\n","==============================\n","TRAIN --> Epoch 5, loss = 0.4352, accuracy = 0.8117, precision = 0.7785, recall = 0.8951, f1 = 0.8272\n","VAL --> Epoch 5, loss = 0.4343, accuracy = 0.8142, precision = 0.7675, recall = 0.8830, f1 = 0.8256\n","==============================\n","TRAIN --> Epoch 6, loss = 0.4318, accuracy = 0.8375, precision = 0.7858, recall = 0.8979, f1 = 0.8357\n","VAL --> Epoch 6, loss = 0.4392, accuracy = 0.8376, precision = 0.7712, recall = 0.8951, f1 = 0.8215\n","==============================\n","TRAIN --> Epoch 7, loss = 0.4330, accuracy = 0.8431, precision = 0.7970, recall = 0.8957, f1 = 0.8417\n","VAL --> Epoch 7, loss = 0.4323, accuracy = 0.8495, precision = 0.7878, recall = 0.8944, f1 = 0.8346\n","==============================\n","TRAIN --> Epoch 8, loss = 0.4339, accuracy = 0.8482, precision = 0.7983, recall = 0.9030, f1 = 0.8563\n","VAL --> Epoch 8, loss = 0.4316, accuracy = 0.8424, precision = 0.7976, recall = 0.8932, f1 = 0.8372\n","==============================\n","TRAIN --> Epoch 9, loss = 0.4232, accuracy = 0.8437, precision = 0.8080, recall = 0.9036, f1 = 0.8588\n","VAL --> Epoch 9, loss = 0.4387, accuracy = 0.8462, precision = 0.7959, recall = 0.9086, f1 = 0.8447\n","==============================\n","TRAIN --> Epoch 10, loss = 0.4294, accuracy = 0.8520, precision = 0.8089, recall = 0.9131, f1 = 0.8541\n","VAL --> Epoch 10, loss = 0.4384, accuracy = 0.8476, precision = 0.7946, recall = 0.9062, f1 = 0.8415\n","==============================\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.semilogy(np.array(range(len(train_total_loss))), train_total_loss)\n","plt.semilogy(np.array(range(len(val_total_loss))), val_total_loss)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('Training loss')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"Sl10VEWHurEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.semilogy(np.array(range(len(train_total_correct))), train_total_correct)\n","plt.semilogy(np.array(range(len(val_total_correct))), val_total_correct)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"N2Hd7bhHvmvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.semilogy(np.array(range(len(train_f1))), train_f1)\n","plt.semilogy(np.array(range(len(val_f1))), val_f1)\n","plt.legend(('Train', 'Test',))\n","plt.ylabel('F1')\n","plt.xlabel('Epoch')\n","plt.show()"],"metadata":{"id":"kJvgytRwvn0q"},"execution_count":null,"outputs":[]}]}